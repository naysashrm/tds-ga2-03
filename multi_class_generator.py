#!/usr/bin/env python
"""
multi_class_generator.py creates a single combined dataset for multi-class classification.
The input for this module are pre-created numpy arrays containing all classes' session 2D histograms,
generated by traffic_csv_converter.py.
"""
import glob
import numpy as np
from sklearn.model_selection import train_test_split
import os


# --- Configuration ---
# Define the root directory where the generated .npy files are located.
# This should match where traffic_csv_converter.py saved them.
# Example: if your .npy files are in ../classes_csvs/Browse/reg/Browse_reg.npy
# then RAW_DATA_DIR should be "../classes_csvs/"
RAW_DATA_DIR = "../classes_csvs/"


# Define the directory where the final combined datasets will be saved.
DATASET_DIR = "../datasets/"


# Define the proportion of the dataset to be used for validation.
TEST_SIZE = 0.1 # 10% for validation, 90% for training


# List of all class names that you want to include in your multi-class dataset.
# These should correspond to your directory names (e.g., 'Browse', 'chat', etc.)
CLASS_NAMES = [
   "Browse",
   "chat",
   "file_transfer",
   "video",
   "voip"
]


# Mapping of class names to numerical labels. Ensure this is consistent with your dataset.
# The order here defines the label (e.g., "Browse" -> 0, "chat" -> 1, etc.)
CLASS_TO_LABEL = {name: i for i, name in enumerate(CLASS_NAMES)}


# --- Helper Functions (re-using from datasets_generator.py) ---


def import_array(input_array_path):
   """Imports a numpy array from a given file path."""
   print(f"Importing dataset: {input_array_path}")
   try:
       dataset = np.load(input_array_path)
       print(f"Dataset shape: {dataset.shape}")
       return dataset
   except FileNotFoundError:
       print(f"Error: File not found at {input_array_path}. Skipping.")
       return None
   except Exception as e:
       print(f"Error loading {input_array_path}: {e}. Skipping.")
       return None


def export_dataset(dataset_dict, file_prefix):
   """Exports datasets from a dictionary to .npy files with a given prefix."""
   os.makedirs(os.path.dirname(file_prefix), exist_ok=True) # Ensure output directory exists
   for name, array in dataset_dict.items():
       output_path = f"{file_prefix}_{name}.npy"
       np.save(output_path, array)
       print(f"Exported {name} dataset to: {output_path} with shape: {array.shape}")


# --- Main Multi-Class Dataset Generation Function ---


def create_multi_class_dataset():
   """
   Creates the combined multi-class dataset from all individual .npy files.
   """
   print("\n--- Starting multi-class dataset generation ---")


   all_x = [] # To store all FlowPic images
   all_y = [] # To store corresponding numerical labels


   # Iterate through each class to find its .npy files
   for class_name in CLASS_NAMES:
       label = CLASS_TO_LABEL[class_name]
       # Find all .npy files for the current class across all VPN types (reg, vpn, tor)
       # Assuming filename is like 'Browse_reg.npy', 'Browse_tor.npy', 'Browse_vpn.npy'
       # The glob pattern should match your actual file naming convention.
       # It looks for files within `RAW_DATA_DIR/<class_name>/*/` ending with '<class_name>_*.npy'
       class_npy_files = glob.glob(f"{RAW_DATA_DIR}{class_name}/*/{class_name}_*.npy")


       if not class_npy_files:
           print(f"No .npy files found for class '{class_name}' in {RAW_DATA_DIR}{class_name}/*/. Skipping this class.")
           continue


       print(f"\nProcessing class: {class_name} (Label: {label})")
       class_data_count = 0
       for npy_file_path in class_npy_files:
           data_array = import_array(npy_file_path)
           if data_array is not None and data_array.size > 0:
               # Append data
               all_x.append(data_array)
               # Create corresponding labels
               labels_for_this_file = np.full(data_array.shape[0], label, dtype=np.int32)
               all_y.append(labels_for_this_file)
               class_data_count += data_array.shape[0]
           else:
               print(f"No valid data extracted from {npy_file_path}.")


       print(f"Total FlowPics collected for '{class_name}': {class_data_count}")


   if not all_x:
       print("[!] No data collected across any classes. Cannot create multi-class dataset. Check your RAW_DATA_DIR and CLASS_NAMES.")
       return


   # Concatenate all collected data and labels
   combined_x = np.concatenate(all_x, axis=0)
   combined_y = np.concatenate(all_y, axis=0)


   print(f"\nTotal combined dataset shape (X): {combined_x.shape}")
   print(f"Total combined labels shape (Y): {combined_y.shape}")


   # Split into training and validation sets
   print(f"Splitting data into training and validation sets with TEST_SIZE={TEST_SIZE}...")
   x_train, x_val, y_train, y_val = train_test_split(
       combined_x, combined_y, test_size=TEST_SIZE, random_state=42, stratify=combined_y
   )


   # Use stratify=combined_y to maintain the proportion of labels in both train and test sets.
   # This is important for imbalanced datasets.


   print(f"x_train shape: {x_train.shape}, y_train shape: {y_train.shape}")
   print(f"x_val shape: {x_val.shape}, y_val shape: {y_val.shape}")


   # Create dataset dictionary for export
   dataset_dict = {
       "x_train": x_train,
       "y_train": y_train,
       "x_val": x_val,
       "y_val": y_val
   }


   # Define the output file prefix
   output_prefix = os.path.join(DATASET_DIR, "overlap_multiclass")


   # Export the datasets
   export_dataset(dataset_dict, output_prefix)
   print("\n--- Multi-class dataset generation complete ---")


# --- Main Execution Block ---
if __name__ == '__main__':
   # Ensure the DATASET_DIR exists before trying to save files
   os.makedirs(DATASET_DIR, exist_ok=True)
   create_multi_class_dataset()

